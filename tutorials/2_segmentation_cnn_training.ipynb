{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with a dataset handy, we'll use it to fit a model.\n",
    "\n",
    "This notebook follows closely the `train.py` scripts found in all of the model directories.\n",
    "First, imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "sys.path.insert(0, '../tfmodels')\n",
    "import tfmodels\n",
    "\n",
    "## Let's use a densnet-small \n",
    "sys.path.insert(0, '../')\n",
    "from densenet_small import Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating and updating the model is simple. First we'll make a bunch of folders to organize the outputs. We'll get back snapshots, logs, and we'll make a few extras for random debugging-related output, and eventually for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_dataset_from_image_mask.ipynb    example_data\r\n",
      "2_segmentation_cnn_training.ipynb  validating_trained_model.ipynb\r\n",
      "deploy_model_to_wsi.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating base experiment directory\n",
      "Creating ./trained_model/logs/2018_07_27_10_34_32\n",
      "Creating ./trained_model/snapshots\n",
      "Creating ./trained_model/debug\n",
      "Creating ./trained_model/inference\n"
     ]
    }
   ],
   "source": [
    "basedir = './trained_model'\n",
    "log_dir, save_dir, debug_dir, infer_dir = tfmodels.make_experiment(basedir=basedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_dataset_from_image_mask.ipynb    example_data\r\n",
      "2_segmentation_cnn_training.ipynb  trained_model\r\n",
      "deploy_model_to_wsi.ipynb\t   validating_trained_model.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set up the paths to the dataset, and a few constants for training. To make everything really lightweight we'll use an input size of 128px. That means we crop out a 512px area and resize it by a factor of 0.25. Also set up how many iterations to train for, and the number of expected classes in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size = 512\n",
    "image_ratio = 0.25\n",
    "record_path = './example_data/image_mask_pairs.tfrecord'\n",
    "iterations = 1000\n",
    "batch_size = 4\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's note a few things. We set record path to be the `tfrecord` object created in the previous notebook. This record has a grand total of 11 image/mask examples in it so we'll probably over fit very quickly. Therefore, we choose a low number of `iterations`, and a small `batch_size`.\n",
    "\n",
    "It might not be so bad for two reasons. First, is the random crop. Each of our 11 examples is $1200 \\times 1200$ pixels, leaving plenty of room for randomness to be introduced just by random cropping. The second helpful technique is hidden from us for now. The class `tfmodels.TFRecordImageMask` dataset (optionally) applies color augmentation to each example it loads. The transformations randomly alter the hue, saturation, and brightness of the image within a set percentage change from the original. Also, they randomly apply flips and rotations. These transformations are valid for our problem because they help a small dataset artificially cover more of the possible distribution of data that exists in the world for this problem. In other words, we never do anything that will make a training image **too** different from a possible naturally occuring image, like one we might see at inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset TRAINING phase\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "dataset = tfmodels.TFRecordImageMask(\n",
    "    training_record = record_path,\n",
    "    sess = sess,\n",
    "    crop_size = crop_size,\n",
    "    ratio = image_ratio,\n",
    "    prefetch = 512, ## How many images to prefetch into memory\n",
    "    shuffle_buffer = 16,\n",
    "    n_classes = 5,\n",
    "    n_threads = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to instantiate the model. Its `Training` mode is already imported, from above. It wants a few variables to be defined in order to set itself up correctly. \n",
    "\n",
    "We require a fixed shape for the input image at training time. This is to help predict how much memory the model will need on GPU. It's also so that we can make sure the input is big enough to repetitively down-sample and not end up with a vector after 2 or 3 convolution-pool layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting 4 dense blocks\n",
      "MINIMIUM DIMENSION:  4\n",
      "Setting up densenet in training mode\n",
      "DenseNet Model\n",
      "Non-linearity: <function selu at 0x7f5d0f653230>\n",
      "\t x_in (?, 128, 128, 3)\n",
      "Dense block #0 (dd)\n",
      "\t Transition Down with k_out= 96\n",
      "Dense block #1 (dd)\n",
      "\t Transition Down with k_out= 144\n",
      "Dense block #2 (dd)\n",
      "\t Transition Down with k_out= 240\n",
      "Dense block #3 (dense)\n",
      "\t Bottleneck:  (?, 4, 4, 528)\n",
      "\t Transition Up with k_out= 264\n",
      "Dense block #0 (du)\n",
      "\t Transition Up with k_out= 96\n",
      "Dense block #1 (du)\n",
      "\t Transition Up with k_out= 48\n",
      "Dense block #2 (du)\n",
      "Model output y_hat: (?, 128, 128, 5)\n",
      "Setting up batch norm update ops\n",
      "Done setting up densenet\n"
     ]
    }
   ],
   "source": [
    "x_dims = [int(crop_size * image_ratio),\n",
    "          int(crop_size * image_ratio),\n",
    "          3]\n",
    "model = Training(\n",
    "    sess = sess,\n",
    "    dataset = dataset,\n",
    "    learning_rate = learning_rate,\n",
    "    log_dir = log_dir,\n",
    "    save_dir = save_dir,\n",
    "    summary_iters = 10, # Log scalars and histograms \n",
    "    summary_image_iters = 100, # Log images\n",
    "    x_dims = x_dims\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model` inherits from `tfmodels.Segmentation`, so there are a couple useful methods baked in. One is `train_step` which is just like it sounds. Pulling a batch from the dataset, the model processes the batch, computes gradients, and applies the gradients via the models optimizer. We can choose the optimizer from any `tf.optimizers`. The default is Adam. The `model` class has an internal counter for the number of times `train_step` is called -- that is used to periodically log. By default the current step and loss are printed whenever a log is written.\n",
    "\n",
    "The second method is `snapshot`. This also does what it sounds like. Given that we instantiated the model with a `save_dir`, we just have to call `model.snapshot()` to save the trainable variables. Be careful not to modify the file with the model code in it, otherwise it might be hard to actually use these snapshots in the future!\n",
    "\n",
    "Run the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshotting to [./trained_model/snapshots/densenet.ckpt] step [1001] ./trained_model/snapshots/densenet.ckpt-1001\n",
      "Done\n",
      "[0001010] writing scalar summaries (loss=0.432) (lr=1.000000E-04)\n",
      "[0001020] writing scalar summaries (loss=0.393) (lr=1.000000E-04)\n",
      "[0001030] writing scalar summaries (loss=0.458) (lr=1.000000E-04)\n",
      "[0001040] writing scalar summaries (loss=0.429) (lr=1.000000E-04)\n",
      "[0001050] writing scalar summaries (loss=0.423) (lr=1.000000E-04)\n",
      "[0001060] writing scalar summaries (loss=0.408) (lr=1.000000E-04)\n",
      "[0001070] writing scalar summaries (loss=0.393) (lr=1.000000E-04)\n",
      "[0001080] writing scalar summaries (loss=0.427) (lr=1.000000E-04)\n",
      "[0001090] writing scalar summaries (loss=0.370) (lr=1.000000E-04)\n",
      "[0001100] writing scalar summaries (loss=0.387) (lr=1.000000E-04)\n",
      "[0001100] writing image summaries\n",
      "[0001110] writing scalar summaries (loss=0.403) (lr=1.000000E-04)\n",
      "[0001120] writing scalar summaries (loss=0.399) (lr=1.000000E-04)\n",
      "[0001130] writing scalar summaries (loss=0.434) (lr=1.000000E-04)\n",
      "[0001140] writing scalar summaries (loss=0.395) (lr=1.000000E-04)\n",
      "[0001150] writing scalar summaries (loss=0.411) (lr=1.000000E-04)\n",
      "[0001160] writing scalar summaries (loss=0.394) (lr=1.000000E-04)\n",
      "[0001170] writing scalar summaries (loss=0.304) (lr=1.000000E-04)\n",
      "[0001180] writing scalar summaries (loss=0.418) (lr=1.000000E-04)\n",
      "[0001190] writing scalar summaries (loss=0.386) (lr=1.000000E-04)\n",
      "[0001200] writing scalar summaries (loss=0.409) (lr=1.000000E-04)\n",
      "[0001200] writing image summaries\n",
      "[0001210] writing scalar summaries (loss=0.392) (lr=1.000000E-04)\n",
      "[0001220] writing scalar summaries (loss=0.403) (lr=1.000000E-04)\n",
      "[0001230] writing scalar summaries (loss=0.389) (lr=1.000000E-04)\n",
      "[0001240] writing scalar summaries (loss=0.369) (lr=1.000000E-04)\n",
      "[0001250] writing scalar summaries (loss=0.449) (lr=1.000000E-04)\n",
      "Snapshotting to [./trained_model/snapshots/densenet.ckpt] step [1251] ./trained_model/snapshots/densenet.ckpt-1251\n",
      "Done\n",
      "[0001260] writing scalar summaries (loss=0.368) (lr=1.000000E-04)\n",
      "[0001270] writing scalar summaries (loss=0.419) (lr=1.000000E-04)\n",
      "[0001280] writing scalar summaries (loss=0.363) (lr=1.000000E-04)\n",
      "[0001290] writing scalar summaries (loss=0.401) (lr=1.000000E-04)\n",
      "[0001300] writing scalar summaries (loss=0.386) (lr=1.000000E-04)\n",
      "[0001300] writing image summaries\n",
      "[0001310] writing scalar summaries (loss=0.482) (lr=1.000000E-04)\n",
      "[0001320] writing scalar summaries (loss=0.391) (lr=1.000000E-04)\n",
      "[0001330] writing scalar summaries (loss=0.319) (lr=1.000000E-04)\n",
      "[0001340] writing scalar summaries (loss=0.415) (lr=1.000000E-04)\n",
      "[0001350] writing scalar summaries (loss=0.356) (lr=1.000000E-04)\n",
      "[0001360] writing scalar summaries (loss=0.395) (lr=1.000000E-04)\n",
      "[0001370] writing scalar summaries (loss=0.368) (lr=1.000000E-04)\n",
      "[0001380] writing scalar summaries (loss=0.315) (lr=1.000000E-04)\n",
      "[0001390] writing scalar summaries (loss=0.378) (lr=1.000000E-04)\n",
      "[0001400] writing scalar summaries (loss=0.348) (lr=1.000000E-04)\n",
      "[0001400] writing image summaries\n",
      "[0001410] writing scalar summaries (loss=0.382) (lr=1.000000E-04)\n",
      "[0001420] writing scalar summaries (loss=0.413) (lr=1.000000E-04)\n",
      "[0001430] writing scalar summaries (loss=0.378) (lr=1.000000E-04)\n",
      "[0001440] writing scalar summaries (loss=0.354) (lr=1.000000E-04)\n",
      "[0001450] writing scalar summaries (loss=0.353) (lr=1.000000E-04)\n",
      "[0001460] writing scalar summaries (loss=0.372) (lr=1.000000E-04)\n",
      "[0001470] writing scalar summaries (loss=0.327) (lr=1.000000E-04)\n",
      "[0001480] writing scalar summaries (loss=0.342) (lr=1.000000E-04)\n",
      "[0001490] writing scalar summaries (loss=0.344) (lr=1.000000E-04)\n",
      "[0001500] writing scalar summaries (loss=0.368) (lr=1.000000E-04)\n",
      "[0001500] writing image summaries\n",
      "Snapshotting to [./trained_model/snapshots/densenet.ckpt] step [1501] ./trained_model/snapshots/densenet.ckpt-1501\n",
      "Done\n",
      "[0001510] writing scalar summaries (loss=0.283) (lr=1.000000E-04)\n",
      "[0001520] writing scalar summaries (loss=0.279) (lr=1.000000E-04)\n",
      "[0001530] writing scalar summaries (loss=0.354) (lr=1.000000E-04)\n",
      "[0001540] writing scalar summaries (loss=0.307) (lr=1.000000E-04)\n",
      "[0001550] writing scalar summaries (loss=0.355) (lr=1.000000E-04)\n",
      "[0001560] writing scalar summaries (loss=0.347) (lr=1.000000E-04)\n",
      "[0001570] writing scalar summaries (loss=0.352) (lr=1.000000E-04)\n",
      "[0001580] writing scalar summaries (loss=0.281) (lr=1.000000E-04)\n",
      "[0001590] writing scalar summaries (loss=0.262) (lr=1.000000E-04)\n",
      "[0001600] writing scalar summaries (loss=0.288) (lr=1.000000E-04)\n",
      "[0001600] writing image summaries\n",
      "[0001610] writing scalar summaries (loss=0.316) (lr=1.000000E-04)\n",
      "[0001620] writing scalar summaries (loss=0.307) (lr=1.000000E-04)\n",
      "[0001630] writing scalar summaries (loss=0.331) (lr=1.000000E-04)\n",
      "[0001640] writing scalar summaries (loss=0.314) (lr=1.000000E-04)\n",
      "[0001650] writing scalar summaries (loss=0.324) (lr=1.000000E-04)\n",
      "[0001660] writing scalar summaries (loss=0.373) (lr=1.000000E-04)\n",
      "[0001670] writing scalar summaries (loss=0.279) (lr=1.000000E-04)\n",
      "[0001680] writing scalar summaries (loss=0.328) (lr=1.000000E-04)\n",
      "[0001690] writing scalar summaries (loss=0.348) (lr=1.000000E-04)\n",
      "[0001700] writing scalar summaries (loss=0.354) (lr=1.000000E-04)\n",
      "[0001700] writing image summaries\n",
      "[0001710] writing scalar summaries (loss=0.423) (lr=1.000000E-04)\n",
      "[0001720] writing scalar summaries (loss=0.286) (lr=1.000000E-04)\n",
      "[0001730] writing scalar summaries (loss=0.363) (lr=1.000000E-04)\n",
      "[0001740] writing scalar summaries (loss=0.243) (lr=1.000000E-04)\n",
      "[0001750] writing scalar summaries (loss=0.304) (lr=1.000000E-04)\n",
      "Snapshotting to [./trained_model/snapshots/densenet.ckpt] step [1751] ./trained_model/snapshots/densenet.ckpt-1751\n",
      "Done\n",
      "[0001760] writing scalar summaries (loss=0.319) (lr=1.000000E-04)\n",
      "[0001770] writing scalar summaries (loss=0.333) (lr=1.000000E-04)\n",
      "[0001780] writing scalar summaries (loss=0.332) (lr=1.000000E-04)\n",
      "[0001790] writing scalar summaries (loss=0.315) (lr=1.000000E-04)\n",
      "[0001800] writing scalar summaries (loss=0.317) (lr=1.000000E-04)\n",
      "[0001800] writing image summaries\n",
      "[0001810] writing scalar summaries (loss=0.354) (lr=1.000000E-04)\n",
      "[0001820] writing scalar summaries (loss=0.317) (lr=1.000000E-04)\n",
      "[0001830] writing scalar summaries (loss=0.327) (lr=1.000000E-04)\n",
      "[0001840] writing scalar summaries (loss=0.421) (lr=1.000000E-04)\n",
      "[0001850] writing scalar summaries (loss=0.272) (lr=1.000000E-04)\n",
      "[0001860] writing scalar summaries (loss=0.305) (lr=1.000000E-04)\n",
      "[0001870] writing scalar summaries (loss=0.339) (lr=1.000000E-04)\n",
      "[0001880] writing scalar summaries (loss=0.304) (lr=1.000000E-04)\n",
      "[0001890] writing scalar summaries (loss=0.349) (lr=1.000000E-04)\n",
      "[0001900] writing scalar summaries (loss=0.334) (lr=1.000000E-04)\n",
      "[0001900] writing image summaries\n",
      "[0001910] writing scalar summaries (loss=0.264) (lr=1.000000E-04)\n",
      "[0001920] writing scalar summaries (loss=0.304) (lr=1.000000E-04)\n",
      "[0001930] writing scalar summaries (loss=0.295) (lr=1.000000E-04)\n",
      "[0001940] writing scalar summaries (loss=0.313) (lr=1.000000E-04)\n",
      "[0001950] writing scalar summaries (loss=0.305) (lr=1.000000E-04)\n",
      "[0001960] writing scalar summaries (loss=0.268) (lr=1.000000E-04)\n",
      "[0001970] writing scalar summaries (loss=0.286) (lr=1.000000E-04)\n",
      "[0001980] writing scalar summaries (loss=0.282) (lr=1.000000E-04)\n",
      "[0001990] writing scalar summaries (loss=0.271) (lr=1.000000E-04)\n",
      "[0002000] writing scalar summaries (loss=0.305) (lr=1.000000E-04)\n",
      "[0002000] writing image summaries\n",
      "Snapshotting to [./trained_model/snapshots/densenet.ckpt] step [2000] ./trained_model/snapshots/densenet.ckpt-2000\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# If this cell is run multiple times, the training will pick up where it left off\n",
    "snapshot_iterations = 250\n",
    "for itx in range(iterations):\n",
    "    model.train_step(lr=learning_rate)\n",
    "    if itx % snapshot_iterations == 0:\n",
    "        model.snapshot()\n",
    "        \n",
    "# Make one final snapshot\n",
    "model.snapshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
